{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-07T07:08:11.697333Z","iopub.execute_input":"2025-11-07T07:08:11.697609Z","iopub.status.idle":"2025-11-07T07:08:11.703794Z","shell.execute_reply.started":"2025-11-07T07:08:11.697586Z","shell.execute_reply":"2025-11-07T07:08:11.702167Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"Source: https://docs.pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html","metadata":{}},{"cell_type":"markdown","source":"# Working with data\nPyTorch has two [primitives to work with data](https://docs.pytorch.org/docs/stable/data.html): `torch.utils.data.DataLoader` and `torch.utils.data.Dataset`. Dataset stores the samples and their corresponding labels, and `DataLoader` wraps an iterable around the `Dataset`.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T07:04:30.066109Z","iopub.execute_input":"2025-11-07T07:04:30.066710Z","iopub.status.idle":"2025-11-07T07:04:30.071667Z","shell.execute_reply.started":"2025-11-07T07:04:30.066664Z","shell.execute_reply":"2025-11-07T07:04:30.070703Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"PyTorch offers domain-specific libraries such as [TorchText](https://pytorch.org/text/stable/index.html), [TorchVision](https://pytorch.org/vision/stable/index.html), and [TorchAudio](https://pytorch.org/audio/stable/index.html), all of which include datasets. For this tutorial, we will be using a TorchVision dataset.\n\nThe `torchvision.datasets` module contains `Dataset` objects for many real-world vision data like CIFAR, COCO ([full list here](https://pytorch.org/vision/stable/datasets.html)). In this tutorial, we use the FashionMNIST dataset. Every TorchVision Dataset includes two arguments: `transform` and `target_transform` to modify the samples and labels respectively.","metadata":{}},{"cell_type":"code","source":"# Download training data from open datasets.\ntraining_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=ToTensor(),\n)\n\n# Download test data from open datasets.\ntest_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=ToTensor(),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T07:04:42.929004Z","iopub.execute_input":"2025-11-07T07:04:42.929330Z","iopub.status.idle":"2025-11-07T07:04:47.346773Z","shell.execute_reply.started":"2025-11-07T07:04:42.929292Z","shell.execute_reply":"2025-11-07T07:04:47.345390Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 26.4M/26.4M [00:01<00:00, 22.2MB/s]\n100%|██████████| 29.5k/29.5k [00:00<00:00, 336kB/s]\n100%|██████████| 4.42M/4.42M [00:00<00:00, 6.26MB/s]\n100%|██████████| 5.15k/5.15k [00:00<00:00, 7.74MB/s]\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"We pass the `Dataset` as an argument to `DataLoader`. This wraps an iterable over our dataset, and supports automatic batching, sampling, shuffling and multiprocess data loading. Here we define a batch size of 64, i.e. each element in the dataloader iterable will return a batch of 64 features and labels.","metadata":{}},{"cell_type":"code","source":"batch_size = 64\n\n# Create data loaders.\ntrain_dataloader = DataLoader(training_data, batch_size=batch_size)\ntest_dataloader = DataLoader(test_data, batch_size=batch_size)\n\nfor X, y in test_dataloader:\n    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n    print(f\"Shape of y: {y.shape} {y.dtype}\")\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T07:04:50.229185Z","iopub.execute_input":"2025-11-07T07:04:50.230440Z","iopub.status.idle":"2025-11-07T07:04:50.258984Z","shell.execute_reply.started":"2025-11-07T07:04:50.230397Z","shell.execute_reply":"2025-11-07T07:04:50.257535Z"}},"outputs":[{"name":"stdout","text":"Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\nShape of y: torch.Size([64]) torch.int64\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"Read more about loading data in PyTorch.","metadata":{}},{"cell_type":"markdown","source":"Creating Models\nTo define a neural network in PyTorch, we create a class that inherits from nn.Module. We define the layers of the network in the `__init__` function and specify how data will pass through the network in the `forward` function. To accelerate operations in the neural network, we move it to the [accelerator](https://pytorch.org/docs/stable/torch.html#accelerators) such as CUDA, MPS, MTIA, or XPU. If the current accelerator is available, we will use it. Otherwise, we use the CPU.","metadata":{}},{"cell_type":"code","source":"device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\nprint(f\"Using {device} device\")\n\n# Define model\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10)\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\nmodel = NeuralNetwork().to(device)\nprint(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T07:04:53.724596Z","iopub.execute_input":"2025-11-07T07:04:53.724921Z","iopub.status.idle":"2025-11-07T07:04:53.739825Z","shell.execute_reply.started":"2025-11-07T07:04:53.724901Z","shell.execute_reply":"2025-11-07T07:04:53.738244Z"}},"outputs":[{"name":"stdout","text":"Using cpu device\nNeuralNetwork(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (linear_relu_stack): Sequential(\n    (0): Linear(in_features=784, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=512, out_features=512, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=512, out_features=10, bias=True)\n  )\n)\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"Read more about [building neural networks in PyTorch](https://docs.pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html).","metadata":{}},{"cell_type":"markdown","source":"# Optimizing the Model Parameters","metadata":{}},{"cell_type":"markdown","source":"To train a model, we need a [loss function](https://pytorch.org/docs/stable/nn.html#loss-functions) and an [optimizer](https://pytorch.org/docs/stable/optim.html).\n\n","metadata":{}},{"cell_type":"code","source":"loss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T07:04:58.435078Z","iopub.execute_input":"2025-11-07T07:04:58.435371Z","iopub.status.idle":"2025-11-07T07:04:58.441193Z","shell.execute_reply.started":"2025-11-07T07:04:58.435351Z","shell.execute_reply":"2025-11-07T07:04:58.440005Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"In a single training loop, the model makes predictions on the training dataset (fed to it in batches), and backpropagates the prediction error to adjust the model’s parameters.","metadata":{}},{"cell_type":"code","source":"def train(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    model.train()\n    for batch, (X, y) in enumerate(dataloader):\n        X, y = X.to(device), y.to(device)\n\n        # Compute prediction error\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # Backpropagation\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), (batch + 1) * len(X)\n            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T07:05:00.857225Z","iopub.execute_input":"2025-11-07T07:05:00.857593Z","iopub.status.idle":"2025-11-07T07:05:00.865002Z","shell.execute_reply.started":"2025-11-07T07:05:00.857572Z","shell.execute_reply":"2025-11-07T07:05:00.863768Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"We also check the model’s performance against the test dataset to ensure it is learning.\n\n","metadata":{}},{"cell_type":"code","source":"def test(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    model.eval()\n    test_loss, correct = 0, 0\n    with torch.no_grad():\n        for X, y in dataloader:\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T07:05:02.832477Z","iopub.execute_input":"2025-11-07T07:05:02.832858Z","iopub.status.idle":"2025-11-07T07:05:02.839763Z","shell.execute_reply.started":"2025-11-07T07:05:02.832829Z","shell.execute_reply":"2025-11-07T07:05:02.838501Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"The training process is conducted over several iterations (epochs). During each epoch, the model learns parameters to make better predictions. We print the model’s accuracy and loss at each epoch; we’d like to see the accuracy increase and the loss decrease with every epoch.","metadata":{}},{"cell_type":"code","source":"epochs = 5\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train(train_dataloader, model, loss_fn, optimizer)\n    test(test_dataloader, model, loss_fn)\nprint(\"Done!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T07:05:04.854576Z","iopub.execute_input":"2025-11-07T07:05:04.854912Z","iopub.status.idle":"2025-11-07T07:06:01.069333Z","shell.execute_reply.started":"2025-11-07T07:05:04.854892Z","shell.execute_reply":"2025-11-07T07:06:01.068347Z"}},"outputs":[{"name":"stdout","text":"Epoch 1\n-------------------------------\nloss: 2.301367  [   64/60000]\nloss: 2.283831  [ 6464/60000]\nloss: 2.262343  [12864/60000]\nloss: 2.251732  [19264/60000]\nloss: 2.235025  [25664/60000]\nloss: 2.210045  [32064/60000]\nloss: 2.213509  [38464/60000]\nloss: 2.182078  [44864/60000]\nloss: 2.178913  [51264/60000]\nloss: 2.147853  [57664/60000]\nTest Error: \n Accuracy: 54.3%, Avg loss: 2.134294 \n\nEpoch 2\n-------------------------------\nloss: 2.150229  [   64/60000]\nloss: 2.140752  [ 6464/60000]\nloss: 2.071564  [12864/60000]\nloss: 2.085661  [19264/60000]\nloss: 2.048927  [25664/60000]\nloss: 1.984088  [32064/60000]\nloss: 2.005648  [38464/60000]\nloss: 1.930786  [44864/60000]\nloss: 1.931136  [51264/60000]\nloss: 1.866806  [57664/60000]\nTest Error: \n Accuracy: 60.3%, Avg loss: 1.853906 \n\nEpoch 3\n-------------------------------\nloss: 1.895684  [   64/60000]\nloss: 1.869725  [ 6464/60000]\nloss: 1.736655  [12864/60000]\nloss: 1.772586  [19264/60000]\nloss: 1.688400  [25664/60000]\nloss: 1.633245  [32064/60000]\nloss: 1.647505  [38464/60000]\nloss: 1.553869  [44864/60000]\nloss: 1.574436  [51264/60000]\nloss: 1.479068  [57664/60000]\nTest Error: \n Accuracy: 61.8%, Avg loss: 1.486150 \n\nEpoch 4\n-------------------------------\nloss: 1.560292  [   64/60000]\nloss: 1.532321  [ 6464/60000]\nloss: 1.365056  [12864/60000]\nloss: 1.438496  [19264/60000]\nloss: 1.341547  [25664/60000]\nloss: 1.326344  [32064/60000]\nloss: 1.342566  [38464/60000]\nloss: 1.265776  [44864/60000]\nloss: 1.296837  [51264/60000]\nloss: 1.210582  [57664/60000]\nTest Error: \n Accuracy: 64.0%, Avg loss: 1.228658 \n\nEpoch 5\n-------------------------------\nloss: 1.305969  [   64/60000]\nloss: 1.299889  [ 6464/60000]\nloss: 1.116313  [12864/60000]\nloss: 1.224102  [19264/60000]\nloss: 1.120861  [25664/60000]\nloss: 1.133928  [32064/60000]\nloss: 1.159536  [38464/60000]\nloss: 1.092382  [44864/60000]\nloss: 1.125762  [51264/60000]\nloss: 1.054734  [57664/60000]\nTest Error: \n Accuracy: 65.2%, Avg loss: 1.071166 \n\nDone!\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"Read more about [Training your model](https://docs.pytorch.org/tutorials/beginner/basics/optimization_tutorial.html).","metadata":{}},{"cell_type":"markdown","source":"# Saving Models","metadata":{}},{"cell_type":"markdown","source":"A common way to save a model is to serialize the internal state dictionary (containing the model parameters).\n\n","metadata":{}},{"cell_type":"code","source":"torch.save(model.state_dict(), \"model.pth\")\nprint(\"Saved PyTorch Model State to model.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T07:06:54.948455Z","iopub.execute_input":"2025-11-07T07:06:54.948815Z","iopub.status.idle":"2025-11-07T07:06:54.960417Z","shell.execute_reply.started":"2025-11-07T07:06:54.948794Z","shell.execute_reply":"2025-11-07T07:06:54.959175Z"}},"outputs":[{"name":"stdout","text":"Saved PyTorch Model State to model.pth\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"# Loading Models","metadata":{}},{"cell_type":"markdown","source":"The process for loading a model includes re-creating the model structure and loading the state dictionary into it.\n\n","metadata":{}},{"cell_type":"code","source":"model = NeuralNetwork().to(device)\nmodel.load_state_dict(torch.load(\"model.pth\", weights_only=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T07:06:59.029864Z","iopub.execute_input":"2025-11-07T07:06:59.030833Z","iopub.status.idle":"2025-11-07T07:06:59.049806Z","shell.execute_reply.started":"2025-11-07T07:06:59.030794Z","shell.execute_reply":"2025-11-07T07:06:59.048298Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":27},{"cell_type":"markdown","source":"This model can now be used to make predictions.\n\n","metadata":{}},{"cell_type":"code","source":"classes = [\n    \"T-shirt/top\",\n    \"Trouser\",\n    \"Pullover\",\n    \"Dress\",\n    \"Coat\",\n    \"Sandal\",\n    \"Shirt\",\n    \"Sneaker\",\n    \"Bag\",\n    \"Ankle boot\",\n]\n\nmodel.eval()\nx, y = test_data[0][0], test_data[0][1]\nwith torch.no_grad():\n    x = x.to(device)\n    pred = model(x)\n    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T07:07:01.934582Z","iopub.execute_input":"2025-11-07T07:07:01.934848Z","iopub.status.idle":"2025-11-07T07:07:01.943754Z","shell.execute_reply.started":"2025-11-07T07:07:01.934830Z","shell.execute_reply":"2025-11-07T07:07:01.942658Z"}},"outputs":[{"name":"stdout","text":"Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"Read more about [Saving & Loading your model](https://docs.pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html).\n\n","metadata":{}}]}